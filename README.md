# Konkani Collector

Web application and API for collecting Amchi Konkani sentence recordings for ASR training.

## Features

- ğŸ“– **Story Management**: Import stories from .txt files, split into sentences
- ğŸ™ï¸ **Web Recorder**: Browser-based recording interface with sentence-by-sentence workflow
- ğŸ”Œ **Programmatic API**: Full API access for external systems (WhatsApp bots, mobile apps)
- âœ… **Audio Validation**: Automatic format/quality checks on upload
- ğŸ“Š **Testing APIs**: Query and download recordings for verification
- ğŸ”„ **ASR Export**: Generate manifest files compatible with `konkani_asr` training pipeline
- â˜ï¸ **Flexible Storage**: Local storage (dev) or cloud (S3/B2/R2) for production

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 konkani_collector                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  1. Story Import (scripts/import-story.js)          â”‚
â”‚     story.txt â†’ Parse â†’ PostgreSQL                   â”‚
â”‚                                                       â”‚
â”‚  2. Recording Collection                             â”‚
â”‚     A. Web UI (public/recorder.html)                â”‚
â”‚     B. Programmatic API (/api/recordings)           â”‚
â”‚                                                       â”‚
â”‚  3. Storage Layer (flexible)                         â”‚
â”‚     - Dev: Local uploads/                            â”‚
â”‚     - Prod: S3/B2/R2                                 â”‚
â”‚                                                       â”‚
â”‚  4. ASR Export (scripts/export-asr-manifest.js)     â”‚
â”‚     PostgreSQL + Storage â†’ konkani_asr/data/        â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Node.js 18+ and npm
- PostgreSQL (Railway)
- ffmpeg (for audio conversion)

### Installation

```bash
cd konkani_collector
npm install
cp .env.example .env
# Edit .env with your DATABASE_URL and storage settings
```

### Database Setup

```bash
# Run schema creation
psql $DATABASE_URL < sql/schema.sql

# Optional: Load example data
psql $DATABASE_URL < sql/seed-example.sql
```

### Running Locally

```bash
# Start server
npm start

# Server runs on http://localhost:3000
# Open http://localhost:3000 in browser
```

### Import a Story

```bash
# Use --title-from-file to avoid PowerShell encoding issues; see "Adding New Stories to Railway" below
node scripts/import-story.js --file story1.txt --title-from-file
# Or with explicit title:
node scripts/import-story.js --file story1.txt --title "à¤ªà¤¾à¤µ à¤µà¤¾à¤Ÿ"
```

### Export for ASR Training

```bash
node scripts/export-asr-manifest.js --output ../konkani_asr/data
```

## API Documentation

### Story Management

```http
GET /api/stories
# List all stories with recording progress
Response: [{ id, title, total_sentences, recorded_count, completion_pct }]

GET /api/stories/:id
# Get story details
Response: { id, title, language, sentences: [...] }
```

### Recording Collection (Web UI)

```http
GET /api/stories/:storyId/next?userId=<session_id>
# Get next unrecorded sentence for user
Response: {
  sentence_id, text_devanagari, text_iast,
  order, total_sentences, remaining
}

POST /api/recordings
# Upload recording (web form)
Form Data: { sentence_id, user_id, audio: File }
Response: { recording_id, status, validation }
```

### Programmatic API (for bots/apps)

```http
POST /api/programmatic/sentence
# Get specific or random sentence
Body: { story_id?: number, sentence_id?: number, random?: boolean }
Response: {
  sentence_id, text_devanagari, text_iast,
  story_title, order_in_story
}

POST /api/programmatic/upload
# Upload recording (JSON + base64 or multipart)
Body: {
  sentence_id: number,
  user_id: string,
  audio: File | base64_string,
  format?: string
}
Response: {
  recording_id, status, audio_duration,
  validation: { valid, errors, checks }
}
```

### Testing & Verification

```http
GET /api/test/health
# Database and system health check
Response: {
  database: "connected",
  storage: "ready",
  total_stories, total_sentences, total_recordings,
  recordings_by_status: { pending, approved, rejected }
}

GET /api/test/sample?status=approved
# Get random sentence-audio pair
Response: {
  sentence: { id, text_devanagari, text_iast, story_title },
  recording: { id, audio_url, duration, sample_rate, file_size }
}

GET /api/test/audio/:recordingId
# Download audio file (streaming)
Response: Binary WAV file

GET /api/test/validate/:recordingId
# Validate specific recording
Response: {
  recording_id, valid, checks: {...}, errors: [...]
}
```

## Storage Configuration

### Local Storage (Development)

```env
STORAGE_TYPE=local
UPLOAD_DIR=./uploads
```

### S3-Compatible Storage (Production)

```env
STORAGE_TYPE=s3

# AWS S3
S3_ENDPOINT=https://s3.amazonaws.com
S3_BUCKET=konkani-recordings
S3_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret

# Or Backblaze B2
S3_ENDPOINT=https://s3.us-west-002.backblazeb2.com
S3_BUCKET=konkani-recordings
S3_REGION=us-west-002
AWS_ACCESS_KEY_ID=<B2_keyId>
AWS_SECRET_ACCESS_KEY=<B2_applicationKey>

# Or Cloudflare R2
S3_ENDPOINT=https://<account_id>.r2.cloudflarestorage.com
S3_BUCKET=konkani-recordings
S3_REGION=auto
AWS_ACCESS_KEY_ID=<R2_access_key>
AWS_SECRET_ACCESS_KEY=<R2_secret_key>
```

## Railway Deployment

1. Push code to GitHub
2. Create new Railway project from GitHub repo
3. Add PostgreSQL service
4. Set environment variables in Railway dashboard
5. Deploy automatically on git push

---

## Adding New Stories to Railway Database

**Important:** Before adding any new story to the Railway database, ensure it is broken into **short sentences**. There must be **no compound sentences**. Long or compound sentences make recording harder and hurt ASR training quality.

### Sentence Rules (No Compound Sentences)

A single database entry = one short sentence. The import script splits text using these rules:

| Rule | Description | Example |
|------|-------------|---------|
| **Devanagari danda** | `à¥¤` and `à¥¥` always end a sentence | `à¤•à¤¾à¤¯à¥à¤³à¥‹ à¤°à¤¾à¤¬à¥à¤¤à¤¾à¤²à¥‹à¥¤ à¤—à¥à¤¬à¥à¤šà¥€ à¤°à¤¾à¤¬à¥à¤¤à¤¾à¤²à¤¿à¥¤` â†’ 2 sentences |
| **Newlines** | Each line break is a boundary | |
| **Period, !, ?** | Outside quotes, these end a sentence | `à¤ à¤•à¥ à¤ à¤•à¥ à¤ à¤•à¥! à¤¬à¤¾à¤—à¤¿à¤² à¤§à¤¾à¤¡à¤¾à¤¯à¥à¤²à¥‡à¤‚à¥¤` â†’ 2 sentences |
| **Speaker changes** | When one speaker's quote ends (closing `"`), the next speaker starts a new sentence | `" à¤•à¥‹à¤£ à¤¤à¥‡à¤‚?"` ends; `à¤•à¤¾à¤¯à¥à¤³à¥‹ ... " à¤¹à¤¾à¤‚à¤µ!"` is a separate sentence |

**Inside quoted dialogue** (`"..."`), periods and `!` `?` do **not** splitâ€”the whole quote is one utterance.

**Bad (compound):**  
`à¤—à¥à¤¬à¥à¤šà¥€à¤¨à¥‡ à¤­à¤¿à¤¤à¥à¤¤à¤°à¥à¤¥à¤¾à¤µà¥à¤¨à¥ à¤¨à¤¿à¤®à¥à¤—à¤¿à¤²à¥‡ - " à¤•à¥‹à¤£ à¤¤à¥‡à¤‚?" à¤•à¤¾à¤¯à¥à¤³à¥‹ à¤•à¤¡à¥à¤•à¤¡à¥à¤¤à¤šà¤¿ à¤®à¥à¤¹à¤³à¤¾à¤²à¥‹ , " à¤¹à¤¾à¤‚à¤µ! à¤•à¤¾à¤¯à¥à¤³à¥‹!"` â€” two speakers in one entry.

**Good (separate entries):**  
1. `à¤—à¥à¤¬à¥à¤šà¥€à¤¨à¥‡ à¤­à¤¿à¤¤à¥à¤¤à¤°à¥à¤¥à¤¾à¤µà¥à¤¨à¥ à¤¨à¤¿à¤®à¥à¤—à¤¿à¤²à¥‡ - " à¤•à¥‹à¤£ à¤¤à¥‡à¤‚?"`  
2. `à¤•à¤¾à¤¯à¥à¤³à¥‹ à¤•à¤¡à¥à¤•à¤¡à¥à¤¤à¤šà¤¿ à¤®à¥à¤¹à¤³à¤¾à¤²à¥‹ , " à¤¹à¤¾à¤‚à¤µ! à¤•à¤¾à¤¯à¥à¤³à¥‹!"`

If your `.txt` file uses these punctuation rules, the import script will split correctly. Otherwise, edit the file to add sentence boundaries (e.g. line breaks, `à¥¤`, `!`, `?`) before importing.

### How to Upload a Story to Railway

1. **Set the Railway database URL**
   ```powershell
   $env:DATABASE_URL = "postgresql://postgres:password@host:port/railway"
   ```
   (Get the connection string from Railway â†’ PostgreSQL service â†’ Connect.)

2. **Import the story** (use `--title-from-file` to avoid PowerShell encoding issues):
   ```powershell
   node scripts/import-story.js --file story6.txt --title-from-file --replace
   ```
   - `--replace` removes an existing story with the same `source_file` and re-imports.

3. **Fix titles if needed** (e.g. after PowerShell corruption):
   ```powershell
   node scripts/fix-story-titles.js
   ```

4. **Verify**
   ```powershell
   node scripts/list-stories.js
   ```

5. **Check on the app**  
   Open https://konkanicollector-production.up.railway.app/ and confirm the story and sentences look correct.

For more detail, see `RAILWAY_STORY_IMPORT_GUIDE.md` and `STORY_IMPORT_PROTECTION.md`.

## Project Structure

```
konkani_collector/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js           # Express app entry
â”‚   â”œâ”€â”€ db.js               # PostgreSQL connection
â”‚   â”œâ”€â”€ storage.js          # Storage abstraction (local/S3)
â”‚   â”œâ”€â”€ routes/             # API endpoints
â”‚   â”œâ”€â”€ middleware/         # Validation, error handling
â”‚   â””â”€â”€ utils/              # Audio conversion, validation
â”œâ”€â”€ scripts/                # CLI tools
â”‚   â”œâ”€â”€ import-story.js
â”‚   â”œâ”€â”€ export-asr-manifest.js
â”‚   â””â”€â”€ verify-audio.js
â”œâ”€â”€ public/                 # Frontend
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ recorder.html
â”‚   â””â”€â”€ app.js
â”œâ”€â”€ sql/                    # Database schema
â”‚   â”œâ”€â”€ schema.sql
â”‚   â””â”€â”€ seed-example.sql
â”œâ”€â”€ tests/                  # Unit tests
â””â”€â”€ uploads/                # Local file storage (gitignored)
```

## Transliteration CLI

Use `scripts/transliterate-sentences.js` to apply transliteration rules to sentences stored in the DB. This tool supports batch processing and checkpointing for long runs.

Flags:
- `--limit <n>`: Limit the number of rows to process (default 5). If `0`, entire table is processed in batches.
- `--all`: Alias for `--limit 0`.
- `--batch-size <n>`: Number of rows to fetch/update per batch (default 100).
- `--start-id <n>`: Start processing from `id > startId`.
- `--checkpoint-file <path>`: Optional; write the `lastId` processed to a JSON checkpoint file for resuming long runs.
- `--dry-run`: Do not perform database updates (report-only).

Examples:
```powershell
# Process 5 rows (default)
node scripts/transliterate-sentences.js --limit 5

# Process whole table in batches of 500 rows and save checkpoint
node scripts/transliterate-sentences.js --limit 0 --batch-size 500 --checkpoint-file trans-checkpoint.json

# Dry-run all rows; helpful to verify counts before writing
node scripts/transliterate-sentences.js --all --batch-size 200 --dry-run
```

## Processing Uploaded Recordings (ffmpeg conversion)

If you need to convert previously uploaded audio files to WAV (16kHz mono) and re-run validation, use the `process-recordings` script.

Prerequisites:
- Install ffmpeg and ffprobe and ensure they are available in the PATH.
  - On Windows (with Chocolatey): `choco install ffmpeg` or download binary from ffmpeg.org
  - On macOS (Homebrew): `brew install ffmpeg`
  - On Linux (apt): `sudo apt-get install ffmpeg` (or use your distro package manager)

Run the script in dry-run mode first, then actual processing:
```powershell
# Dry-run to preview what will be changed
node scripts/process-recordings.js --limit 100 --dry-run

# Process (make changes)
node scripts/process-recordings.js --limit 100
```

Notes:
- The script downloads the files from storage, converts non-WAV files to WAV (if ffmpeg is present), runs validation using ffprobe, then saves the new WAV file and updates the DB record with new metadata.
- Use `--batch-size` and `--start-id` to manage long runs and resuming.


## Development Workflow

1. **Import stories**: Ensure short sentences (no compounds)â€”see "Adding New Stories to Railway Database". Then: `node scripts/import-story.js --file story.txt --title-from-file`
2. **Start server**: `npm start`
3. **Record audio**: Open http://localhost:3000/recorder.html
4. **Verify data**: `curl http://localhost:3000/api/test/health`
5. **Export for ASR**: `node scripts/export-asr-manifest.js`
6. **Train model**: `cd ../konkani_asr && python scripts/fine_tune_hf.py`

## Testing

```bash
# Run all tests
npm test

# Test specific component
npm test -- import-story.test.js

# Integration tests
npm run test:integration
```

## License

MIT
